# -*- coding: utf-8 -*-

import streamlit as st
import google.generativeai as genai
import json
import os

# --- C·∫•u h√¨nh ·ª©ng d·ª•ng ---
st.set_page_config(page_title="INNO HR Chatbot", page_icon="ü§ñ")
st.title("ü§ñ Tr·ª£ l√Ω AI H·ªèi ƒê√°p Nh√¢n S·ª± INNO")
st.caption("H·ªèi ƒë√°p d·ª±a tr√™n d·ªØ li·ªáu n·ªôi b·ªô (JSON)")

# --- CSS t√πy ch·ªânh ƒë·ªÉ c·ªë ƒë·ªãnh √¥ nh·∫≠p li·ªáu ·ªü cu·ªëi ---
# L∆∞u √Ω: CSS n√†y d·ª±a tr√™n c·∫•u tr√∫c HTML hi·ªán t·∫°i c·ªßa Streamlit v√† c√≥ th·ªÉ c·∫ßn
# ƒëi·ªÅu ch·ªânh n·∫øu Streamlit c·∫≠p nh·∫≠t c·∫•u tr√∫c trong t∆∞∆°ng lai.
st.markdown("""
<style>
/* Target the container holding the text input */
/* Adjust selector if needed based on Streamlit version changes */
div.stTextInput {
    position: fixed; /* C·ªë ƒë·ªãnh v·ªã tr√≠ */
    bottom: 1rem; /* Kho·∫£ng c√°ch t·ª´ ƒë√°y m√†n h√¨nh */
    left: 50%; /* CƒÉn gi·ªØa theo chi·ªÅu ngang */
    transform: translateX(-50%); /* ƒêi·ªÅu ch·ªânh cƒÉn gi·ªØa ch√≠nh x√°c */
    width: calc(100% - 4rem); /* Chi·ªÅu r·ªông, tr·ª´ ƒëi padding hai b√™n */
    max-width: 736px; /* Gi·ªõi h·∫°n chi·ªÅu r·ªông t·ªëi ƒëa gi·ªëng n·ªôi dung ch√≠nh */
    padding: 0.75rem 1rem;
    background-color: #ffffff; /* M√†u n·ªÅn tr·∫Øng (ho·∫∑c m√†u n·ªÅn theme) */
    border-top: 1px solid #e6e6e6; /* ƒê∆∞·ªùng vi·ªÅn m·ªù ph√≠a tr√™n */
    border-radius: 0.5rem; /* Bo g√≥c nh·∫π */
    box-shadow: 0 -2px 5px rgba(0,0,0,0.05); /* ƒê·ªï b√≥ng nh·∫π */
    z-index: 99; /* ƒê·∫£m b·∫£o n·∫±m tr√™n c√°c th√†nh ph·∫ßn kh√°c */
}

/* Th√™m padding v√†o cu·ªëi n·ªôi dung ch√≠nh ƒë·ªÉ kh√¥ng b·ªã √¥ input che m·∫•t */
.main .block-container {
    padding-bottom: 6rem; /* TƒÉng padding ƒë·ªÉ ƒë·ªß ch·ªó cho √¥ input c·ªë ƒë·ªãnh */
}
</style>
""", unsafe_allow_html=True)

# --- T·∫£i d·ªØ li·ªáu nh√¢n s·ª± t·ª´ file JSON ---
JSON_FILE_PATH = "data.json"

@st.cache_data
def load_knowledge(file_path):
    """T·∫£i d·ªØ li·ªáu JSON m√† kh√¥ng chi·∫øm qu√° nhi·ªÅu b·ªô nh·ªõ"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)  # Tr·∫£ v·ªÅ object JSON thay v√¨ string
    except FileNotFoundError:
        st.sidebar.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file '{file_path}'!")
        return None
    except json.JSONDecodeError:
        st.sidebar.error(f"‚ùå File '{file_path}' kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng JSON!")
        return None
    except Exception as e:
        st.sidebar.error(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu: {e}")
        return None

knowledge_text = load_knowledge(JSON_FILE_PATH)

if knowledge_text is None:
    st.error("Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông ·ª©ng d·ª•ng do l·ªói nghi√™m tr·ªçng khi t·∫£i d·ªØ li·ªáu ki·∫øn th·ª©c. Vui l√≤ng ki·ªÉm tra file data.json v√† th·ª≠ l·∫°i.")
    st.stop()
else:
    st.sidebar.success(f"ƒê√£ t·∫£i th√†nh c√¥ng d·ªØ li·ªáu t·ª´ {JSON_FILE_PATH}")

# --- C·∫•u h√¨nh Google Generative AI ---
api_key = os.getenv("GEMINI_API_KEY") or st.secrets.get("GOOGLE_API_KEY", None)

if not api_key:
    st.error("L·ªói: API Key ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh. Vui l√≤ng ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng GEMINI_API_KEY ho·∫∑c c·∫•u h√¨nh GOOGLE_API_KEY trong Streamlit Secrets.")
    st.stop()

try:
    genai.configure(api_key=api_key)
except Exception as e:
    st.error(f"L·ªói khi c·∫•u h√¨nh Google AI v·ªõi API Key: {e}")
    st.stop()

MODEL_NAME = "gemini-2.5-pro-exp-03-25"  # Ho·∫∑c 'gemini-pro' n·∫øu c·∫ßn

try:
    model = genai.GenerativeModel(MODEL_NAME)
    st.sidebar.info(f"S·ª≠ d·ª•ng model: {MODEL_NAME}")
except Exception as e:
    st.error(f"L·ªói khi kh·ªüi t·∫°o model '{MODEL_NAME}': {e}. C√≥ th·ªÉ model kh√¥ng t·ªìn t·∫°i ho·∫∑c API key kh√¥ng h·ª£p l·ªá/kh√¥ng c√≥ quy·ªÅn truy c·∫≠p.")
    st.stop()

# --- System Prompt ---
system_instruction_text = """
Act as the HR information manager of INNO company. You have the skills and knowledge to read, remember, check, and respond to HR-related inquiries. When asked for information, you must only use the provided data to answer the question.

If the information is not available in the provided data, respond with: "I could not find this information in the available data."
Do not make guesses or provide uncertain information.

Your process for handling and responding to inquiries is as follows:

- Receive the question
- Analyze and understand the question and the user's concern in natural language
- (Important) Carefully check the information in the provided documents and extract all relevant and accurate details
- Respond in natural language
- Accept feedback if the response is incorrect.
"""

# --- C·∫•u h√¨nh t·∫°o n·ªôi dung ---
generation_config = genai.types.GenerationConfig(
    response_mime_type="text/plain",
    temperature=0.7  # Gi·ªØ nguy√™n nhi·ªát ƒë·ªô
)

# --- X·ª≠ l√Ω ph·∫£n h·ªìi d·∫°ng Stream ---
def stream_text_generator(stream):
    """Nh·∫≠n stream t·ª´ API v√† ch·ªâ xu·∫•t ph·∫ßn text"""
    for chunk in stream:
        if hasattr(chunk, 'text') and chunk.text:
            yield chunk.text

# --- Qu·∫£n l√Ω L·ªãch s·ª≠ Chat (Session State) ---
# Kh·ªüi t·∫°o danh s√°ch tin nh·∫Øn n·∫øu ch∆∞a c√≥
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- Hi·ªÉn th·ªã L·ªãch s·ª≠ Chat ---
st.markdown("### L·ªãch s·ª≠ tr√≤ chuy·ªán")
# T·∫°o container ƒë·ªÉ ch·ª©a l·ªãch s·ª≠, cho ph√©p cu·ªôn n·∫øu c·∫ßn
# Kh√¥ng c·∫ßn gi·ªõi h·∫°n chi·ªÅu cao c·ªë ƒë·ªãnh n·∫øu √¥ input ƒë√£ c·ªë ƒë·ªãnh ·ªü d∆∞·ªõi
chat_container = st.container()

with chat_container:
    # Ch·ªâ hi·ªÉn th·ªã t·ªëi ƒëa 2 c·∫∑p h·ªèi ƒë√°p g·∫ßn nh·∫•t (4 tin nh·∫Øn)
    history_limit = 4
    start_index = max(0, len(st.session_state.messages) - history_limit)
    for i in range(start_index, len(st.session_state.messages)):
        message = st.session_state.messages[i]
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# --- Giao di·ªán nh·∫≠p c√¢u h·ªèi (S·∫Ω ƒë∆∞·ª£c CSS di chuy·ªÉn xu·ªëng d∆∞·ªõi) ---
user_question = st.text_input(
    "Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:",
    key="user_question_input", # Key ƒë·ªÉ qu·∫£n l√Ω input
    placeholder="V√≠ d·ª•: Cho t√¥i bi·∫øt email c·ªßa Nguy·ªÖn VƒÉn A?",
    label_visibility="collapsed" # ·∫®n label m·∫∑c ƒë·ªãnh ƒë·ªÉ ti·∫øt ki·ªám kh√¥ng gian
)

# --- X·ª≠ l√Ω logic ch√≠nh khi c√≥ c√¢u h·ªèi m·ªõi ---
if user_question:
    # 1. Th√™m c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠ v√† hi·ªÉn th·ªã ngay l·∫≠p t·ª©c
    # Ki·ªÉm tra xem tin nh·∫Øn cu·ªëi c√πng c√≥ ph·∫£i l√† c√¢u h·ªèi n√†y kh√¥ng ƒë·ªÉ tr√°nh l·∫∑p l·∫°i khi rerun
    # ----- S·ª¨A L·ªñI: TO√ÄN B·ªò LOGIC B√äN D∆Ø·ªöI PH·∫¢I N·∫∞M TRONG KH·ªêI IF N√ÄY -----
    if not st.session_state.messages or st.session_state.messages[-1].get("content") != user_question:
        st.session_state.messages.append({"role": "user", "content": user_question})

        # Hi·ªÉn th·ªã c√¢u h·ªèi m·ªõi nh·∫•t trong container l·ªãch s·ª≠
        # ----- Th·ª•t l·ªÅ ƒë√∫ng -----
        with chat_container:
            with st.chat_message("user"):
                st.markdown(user_question)

        # ----- Th·ª•t l·ªÅ ƒë√∫ng cho to√†n b·ªô kh·ªëi x·ª≠ l√Ω prompt v√† API -----
        # 2. X√¢y d·ª±ng prompt
        # Chuy·ªÉn JSON th√†nh chu·ªói (l∆∞u √Ω: indent=2 s·∫Ω t·ªën nhi·ªÅu token h∆°n indent=None)
        knowledge_base_string = json.dumps(knowledge_text, ensure_ascii=False, indent=None) # <<< S·ª≠a l·∫°i indent=None ƒë·ªÉ ti·∫øt ki·ªám token
        full_prompt = f"{system_instruction_text}\n\nD∆∞·ªõi ƒë√¢y l√† d·ªØ li·ªáu nh√¢n s·ª±:\n\n{knowledge_base_string}\n\nH√£y tr·∫£ l·ªùi c√¢u h·ªèi sau:\n\"{user_question}\""

        # ----- TH√äM L·∫†I D√íNG B·ªä THI·∫æU V√Ä TH·ª§T L·ªÄ ƒê√öNG -----
        contents = [full_prompt]

        # 3. G·ªçi API v√† x·ª≠ l√Ω ph·∫£n h·ªìi
        try:
            # ----- Th·ª•t l·ªÅ ƒë√∫ng -----
            with chat_container:
                with st.spinner(f"üîç ƒêang t√¨m ki·∫øm c√¢u tr·∫£ l·ªùi v·ªõi {MODEL_NAME}..."):
                    response_stream = model.generate_content(
                        contents, # <<< S·ª≠ d·ª•ng bi·∫øn contents ƒë√£ ƒë·ªãnh nghƒ©a
                        stream=True,
                        generation_config=generation_config
                    )

                    # 4. Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi d·∫°ng stream v√† l∆∞u tr·ªØ
                    with st.chat_message("assistant"):
                        full_response = ""
                        text_stream_placeholder = st.empty()
                        text_generator = stream_text_generator(response_stream)
                        for chunk in text_generator:
                            full_response += chunk
                            text_stream_placeholder.markdown(full_response + "‚ñå") # Con tr·ªè nh·∫•p nh√°y
                        text_stream_placeholder.markdown(full_response) # Hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß

            # 5. Th√™m c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß v√†o l·ªãch s·ª≠ (sau khi stream xong)
            # ----- Th·ª•t l·ªÅ ƒë√∫ng -----
            st.session_state.messages.append({"role": "assistant", "content": full_response})

            # C√°c d√≤ng t√πy ch·ªçn kh√°c gi·ªØ nguy√™n th·ª•t l·ªÅ n√†y
            # st.session_state.user_question_input = ""
            # st.rerun()

        except Exception as e:
             # ----- Th·ª•t l·ªÅ ƒë√∫ng -----
            st.error(f"üö® ƒê√£ x·∫£y ra l·ªói khi g·ªçi Gemini API: {e}")
            print(f"Error calling Gemini API: {e}")

# --- Kh√¥ng c·∫ßn else cho if not st.session_state.messages... v√¨ ch·ªâ x·ª≠ l√Ω khi c√≥ c√¢u h·ªèi M·ªöI ---

# --- Kh·ªëi elif n√†y ƒë·ªÉ x·ª≠ l√Ω khi √¥ input tr·ªëng ---
# elif not user_question: # Kh√¥ng c·∫ßn thi·∫øt v√¨ ƒë√£ c√≥ x·ª≠ l√Ω hi·ªÉn th·ªã l·ªãch s·ª≠ ·ªü tr√™n
#     pass

# --- Th√¥ng tin th√™m ---
# C√≥ th·ªÉ kh√¥ng c·∫ßn thi·∫øt n·ªØa n·∫øu giao di·ªán t·∫≠p trung v√†o chat
# st.sidebar.markdown("---")
# st.sidebar.caption("¬© 2025 - Beta App v0.7") # C·∫≠p nh·∫≠t version
