# -*- coding: utf-8 -*-

import streamlit as st
import google.generativeai as genai
import json
import os

# --- C·∫•u h√¨nh ·ª©ng d·ª•ng ---
st.set_page_config(page_title="INNO HR Chatbot", page_icon="ü§ñ")
st.title("ü§ñ Tr·ª£ l√Ω AI H·ªèi ƒê√°p Nh√¢n S·ª± INNO")
st.caption("H·ªèi ƒë√°p d·ª±a tr√™n d·ªØ li·ªáu n·ªôi b·ªô (JSON)")

# --- CSS t√πy ch·ªânh ƒë·ªÉ c·ªë ƒë·ªãnh √¥ nh·∫≠p li·ªáu ·ªü cu·ªëi ---
# L∆∞u √Ω: CSS n√†y d·ª±a tr√™n c·∫•u tr√∫c HTML hi·ªán t·∫°i c·ªßa Streamlit v√† c√≥ th·ªÉ c·∫ßn
# ƒëi·ªÅu ch·ªânh n·∫øu Streamlit c·∫≠p nh·∫≠t c·∫•u tr√∫c trong t∆∞∆°ng lai.
st.markdown("""
<style>
/* Target the container holding the text input */
/* Adjust selector if needed based on Streamlit version changes */
div.stTextInput {
    position: fixed; /* C·ªë ƒë·ªãnh v·ªã tr√≠ */
    bottom: 1rem; /* Kho·∫£ng c√°ch t·ª´ ƒë√°y m√†n h√¨nh */
    left: 50%; /* CƒÉn gi·ªØa theo chi·ªÅu ngang */
    transform: translateX(-50%); /* ƒêi·ªÅu ch·ªânh cƒÉn gi·ªØa ch√≠nh x√°c */
    width: calc(100% - 4rem); /* Chi·ªÅu r·ªông, tr·ª´ ƒëi padding hai b√™n */
    max-width: 736px; /* Gi·ªõi h·∫°n chi·ªÅu r·ªông t·ªëi ƒëa gi·ªëng n·ªôi dung ch√≠nh */
    padding: 0.75rem 1rem;
    background-color: #ffffff; /* M√†u n·ªÅn tr·∫Øng (ho·∫∑c m√†u n·ªÅn theme) */
    border-top: 1px solid #e6e6e6; /* ƒê∆∞·ªùng vi·ªÅn m·ªù ph√≠a tr√™n */
    border-radius: 0.5rem; /* Bo g√≥c nh·∫π */
    box-shadow: 0 -2px 5px rgba(0,0,0,0.05); /* ƒê·ªï b√≥ng nh·∫π */
    z-index: 99; /* ƒê·∫£m b·∫£o n·∫±m tr√™n c√°c th√†nh ph·∫ßn kh√°c */
}

/* Th√™m padding v√†o cu·ªëi n·ªôi dung ch√≠nh ƒë·ªÉ kh√¥ng b·ªã √¥ input che m·∫•t */
.main .block-container {
    padding-bottom: 6rem; /* TƒÉng padding ƒë·ªÉ ƒë·ªß ch·ªó cho √¥ input c·ªë ƒë·ªãnh */
}
</style>
""", unsafe_allow_html=True)

# --- T·∫£i d·ªØ li·ªáu nh√¢n s·ª± t·ª´ file JSON ---
JSON_FILE_PATH = "data.json"

@st.cache_data
def load_knowledge(file_path):
    """T·∫£i d·ªØ li·ªáu JSON m√† kh√¥ng chi·∫øm qu√° nhi·ªÅu b·ªô nh·ªõ"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)  # Tr·∫£ v·ªÅ object JSON thay v√¨ string
    except FileNotFoundError:
        st.sidebar.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file '{file_path}'!")
        return None
    except json.JSONDecodeError:
        st.sidebar.error(f"‚ùå File '{file_path}' kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng JSON!")
        return None
    except Exception as e:
        st.sidebar.error(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu: {e}")
        return None

knowledge_text = load_knowledge(JSON_FILE_PATH)

if knowledge_text is None:
    st.error("Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông ·ª©ng d·ª•ng do l·ªói nghi√™m tr·ªçng khi t·∫£i d·ªØ li·ªáu ki·∫øn th·ª©c. Vui l√≤ng ki·ªÉm tra file data.json v√† th·ª≠ l·∫°i.")
    st.stop()
else:
    st.sidebar.success(f"ƒê√£ t·∫£i th√†nh c√¥ng d·ªØ li·ªáu t·ª´ {JSON_FILE_PATH}")

# --- C·∫•u h√¨nh Google Generative AI ---
api_key = os.getenv("GEMINI_API_KEY") or st.secrets.get("GOOGLE_API_KEY", None)

if not api_key:
    st.error("L·ªói: API Key ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh. Vui l√≤ng ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng GEMINI_API_KEY ho·∫∑c c·∫•u h√¨nh GOOGLE_API_KEY trong Streamlit Secrets.")
    st.stop()

try:
    genai.configure(api_key=api_key)
except Exception as e:
    st.error(f"L·ªói khi c·∫•u h√¨nh Google AI v·ªõi API Key: {e}")
    st.stop()

MODEL_NAME = "gemini-2.5-pro-exp-03-25"  # Ho·∫∑c 'gemini-pro' n·∫øu c·∫ßn

try:
    model = genai.GenerativeModel(MODEL_NAME)
    st.sidebar.info(f"S·ª≠ d·ª•ng model: {MODEL_NAME}")
except Exception as e:
    st.error(f"L·ªói khi kh·ªüi t·∫°o model '{MODEL_NAME}': {e}. C√≥ th·ªÉ model kh√¥ng t·ªìn t·∫°i ho·∫∑c API key kh√¥ng h·ª£p l·ªá/kh√¥ng c√≥ quy·ªÅn truy c·∫≠p.")
    st.stop()

# --- System Prompt ---
system_instruction_text = """
Act as the HR information manager of INNO company. You have the skills and knowledge to read, remember, check, and respond to HR-related inquiries. When asked for information, you must only use the provided data to answer the question.

If the information is not available in the provided data, respond with: "I could not find this information in the available data."
Do not make guesses or provide uncertain information.

Your process for handling and responding to inquiries is as follows:

- Receive the question
- Analyze and understand the question and the user's concern in natural language
- (Important) Carefully check the information in the provided documents and extract all relevant and accurate details
- Respond in natural language
- Accept feedback if the response is incorrect.
"""

# --- C·∫•u h√¨nh t·∫°o n·ªôi dung ---
generation_config = genai.types.GenerationConfig(
    response_mime_type="text/plain",
    temperature=0.5  # Gi·ªØ nguy√™n nhi·ªát ƒë·ªô
)

# --- X·ª≠ l√Ω ph·∫£n h·ªìi d·∫°ng Stream ---
def stream_text_generator(stream):
    """Nh·∫≠n stream t·ª´ API v√† ch·ªâ xu·∫•t ph·∫ßn text"""
    for chunk in stream:
        if hasattr(chunk, 'text') and chunk.text:
            yield chunk.text

# --- Qu·∫£n l√Ω L·ªãch s·ª≠ Chat (Session State) ---
# Kh·ªüi t·∫°o danh s√°ch tin nh·∫Øn n·∫øu ch∆∞a c√≥
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- Hi·ªÉn th·ªã L·ªãch s·ª≠ Chat ---
st.markdown("### L·ªãch s·ª≠ tr√≤ chuy·ªán")
chat_container = st.container()

with chat_container:
    # Gi·ªõi h·∫°n hi·ªÉn th·ªã 2 c·∫∑p h·ªèi ƒë√°p g·∫ßn nh·∫•t (4 tin nh·∫Øn) - C√≥ th·ªÉ ƒë·ªïi l·∫°i th√†nh 6 n·∫øu mu·ªën 3 c·∫∑p
    history_display_limit = 4
    start_index = max(0, len(st.session_state.messages) - history_display_limit)
    for i in range(start_index, len(st.session_state.messages)):
        message = st.session_state.messages[i]
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# --- Giao di·ªán nh·∫≠p c√¢u h·ªèi (S·∫Ω ƒë∆∞·ª£c CSS di chuy·ªÉn xu·ªëng d∆∞·ªõi) ---
user_question = st.text_input(
    "Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:",
    key="user_question_input",
    placeholder="V√≠ d·ª•: Cho t√¥i bi·∫øt email c·ªßa Nguy·ªÖn VƒÉn A?",
    label_visibility="collapsed"
)

# --- X·ª≠ l√Ω logic ch√≠nh khi c√≥ c√¢u h·ªèi m·ªõi ---
if user_question:
    # Ch·ªâ x·ª≠ l√Ω n·∫øu c√¢u h·ªèi m·ªõi kh√°c c√¢u h·ªèi cu·ªëi c√πng trong l·ªãch s·ª≠
    if not st.session_state.messages or st.session_state.messages[-1].get("content") != user_question or st.session_state.messages[-1].get("role") != "user":
        # 1. Th√™m c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠ v√† hi·ªÉn th·ªã ngay
        st.session_state.messages.append({"role": "user", "content": user_question})
        with chat_container:
            with st.chat_message("user"):
                st.markdown(user_question)

        # 2. X√¢y d·ª±ng `contents` cho API bao g·ªìm c·∫£ l·ªãch s·ª≠
        knowledge_base_string = json.dumps(knowledge_text, ensure_ascii=False, indent=None)

        # --- THAY ƒê·ªîI QUAN TR·ªåNG: Chu·∫©n b·ªã n·ªôi dung g·ª≠i cho API ---
        contents_for_api = []

        # L·∫•y l·ªãch s·ª≠ g·∫ßn nh·∫•t ƒë·ªÉ g·ª≠i cho model (v√≠ d·ª•: 3 c·∫∑p = 6 tin nh·∫Øn)
        # L·∫•y nhi·ªÅu h∆°n 1 so v·ªõi s·ªë l∆∞·ª£ng hi·ªÉn th·ªã ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªß ng·ªØ c·∫£nh
        history_model_limit = 6
        # L·∫•y c√°c tin nh·∫Øn TR∆Ø·ªöC c√¢u h·ªèi hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng
        start_index_model = max(0, len(st.session_state.messages) - 1 - history_model_limit)
        # Ch·ªâ l·∫•y ƒë·∫øn tin nh·∫Øn ngay tr∆∞·ªõc tin nh·∫Øn user v·ª´a th√™m v√†o
        recent_history = st.session_state.messages[start_index_model:-1]

        # Th√™m l·ªãch s·ª≠ v√†o contents_for_api v·ªõi ƒë√∫ng ƒë·ªãnh d·∫°ng role/parts
        for msg in recent_history:
            role = "model" if msg["role"] == "assistant" else "user"
            contents_for_api.append({"role": role, "parts": [{"text": msg["content"]}]})

        # T·∫°o prompt cho l∆∞·ª£t hi·ªán t·∫°i, bao g·ªìm system instruction, knowledge base, v√† c√¢u h·ªèi M·ªöI
        current_prompt = f"{system_instruction_text}\n\nD∆∞·ªõi ƒë√¢y l√† d·ªØ li·ªáu nh√¢n s·ª±:\n\n{knowledge_base_string}\n\nH√£y tr·∫£ l·ªùi c√¢u h·ªèi sau d·ª±a v√†o d·ªØ li·ªáu v√† ng·ªØ c·∫£nh cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥ (n·∫øu c√≥):\n\"{user_question}\""
        # Th√™m prompt hi·ªán t·∫°i v√†o cu·ªëi danh s√°ch contents
        contents_for_api.append({"role": "user", "parts": [{"text": current_prompt}]})
        # --------------------------------------------------------------

        # 3. G·ªçi API v√† x·ª≠ l√Ω ph·∫£n h·ªìi
        try:
            with chat_container:
                with st.spinner(f"üîç ƒêang t√¨m ki·∫øm c√¢u tr·∫£ l·ªùi v·ªõi {MODEL_NAME}..."):
                    response_stream = model.generate_content(
                        contents_for_api, # <<< S·ª≠ d·ª•ng contents ƒë√£ bao g·ªìm l·ªãch s·ª≠
                        stream=True,
                        generation_config=generation_config
                    )

                    # 4. Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi d·∫°ng stream v√† l∆∞u tr·ªØ
                    with st.chat_message("assistant"):
                        full_response = ""
                        text_stream_placeholder = st.empty()
                        text_generator = stream_text_generator(response_stream)
                        try:
                            for chunk in text_generator:
                                full_response += chunk
                                text_stream_placeholder.markdown(full_response + "‚ñå")
                            text_stream_placeholder.markdown(full_response) # Hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß khi xong
                        except Exception as stream_error:
                            # B·∫Øt l·ªói c√≥ th·ªÉ x·∫£y ra trong qu√° tr√¨nh stream (v√≠ d·ª•: b·ªã ch·∫∑n gi·ªØa ch·ª´ng)
                            st.error(f"üö® L·ªói trong qu√° tr√¨nh nh·∫≠n ph·∫£n h·ªìi: {stream_error}")
                            print(f"Error during streaming response: {stream_error}")
                            full_response += f"\n\n[L·ªói: Kh√¥ng th·ªÉ ho√†n th√†nh c√¢u tr·∫£ l·ªùi - {stream_error}]" # Ghi nh·∫≠n l·ªói v√†o n·ªôi dung
                            text_stream_placeholder.markdown(full_response) # C·∫≠p nh·∫≠t l·∫ßn cu·ªëi

            # 5. Th√™m c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß v√†o l·ªãch s·ª≠ (sau khi stream xong ho·∫∑c g·∫∑p l·ªói stream)
            if full_response or 'stream_error' in locals(): # Ch·ªâ l∆∞u n·∫øu c√≥ n·ªôi dung ho·∫∑c c√≥ l·ªói stream
                 st.session_state.messages.append({"role": "assistant", "content": full_response})

            # 6. T√πy ch·ªçn x√≥a input ho·∫∑c rerun
            # st.session_state.user_question_input = ""
            # st.rerun()

        except Exception as e:
             st.error(f"üö® ƒê√£ x·∫£y ra l·ªói khi g·ªçi Gemini API: {e}")
             print(f"Error calling Gemini API: {e}")

# --- Kh·ªëi elif n√†y ƒë·ªÉ x·ª≠ l√Ω khi √¥ input tr·ªëng ---
# elif not user_question: # Kh√¥ng c·∫ßn thi·∫øt v√¨ ƒë√£ c√≥ x·ª≠ l√Ω hi·ªÉn th·ªã l·ªãch s·ª≠ ·ªü tr√™n
#     pass

# --- Th√¥ng tin th√™m ---
# C√≥ th·ªÉ kh√¥ng c·∫ßn thi·∫øt n·ªØa n·∫øu giao di·ªán t·∫≠p trung v√†o chat
# st.sidebar.markdown("---")
# st.sidebar.caption("¬© 2025 - Beta App v0.7") # C·∫≠p nh·∫≠t version
