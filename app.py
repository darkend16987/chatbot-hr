# -*- coding: utf-8 -*-
import streamlit as st
import google.generativeai as genai
import json
import os
import pandas as pd # Giá»¯ láº¡i import pandas náº¿u báº¡n váº«n dÃ¹ng CSV trong tÆ°Æ¡ng lai

# --- Cáº¥u hÃ¬nh á»©ng dá»¥ng ---
# Sá»­ dá»¥ng layout="wide" cÃ³ thá»ƒ khÃ´ng cáº§n thiáº¿t ná»¯a khi cÃ³ CSS tÃ¹y chá»‰nh
st.set_page_config(page_title="INNO HR Chatbot", page_icon="ğŸ¤–")
# st.title("ğŸ¤– Beta App Há»i ÄÃ¡p NhÃ¢n Sá»± INNO") # TiÃªu Ä‘á» cÃ³ thá»ƒ áº©n Ä‘i Ä‘á»ƒ giá»‘ng chat hÆ¡n
# st.caption("Há»i Ä‘Ã¡p dá»±a trÃªn dá»¯ liá»‡u ná»™i bá»™ (JSON)") # Caption cÃ³ thá»ƒ áº©n

# --- CSS tÃ¹y chá»‰nh Ä‘á»ƒ cá»‘ Ä‘á»‹nh Ã´ nháº­p liá»‡u á»Ÿ cuá»‘i ---
# LÆ°u Ã½: CSS nÃ y dá»±a trÃªn cáº¥u trÃºc HTML hiá»‡n táº¡i cá»§a Streamlit vÃ  cÃ³ thá»ƒ cáº§n
# Ä‘iá»u chá»‰nh náº¿u Streamlit cáº­p nháº­t cáº¥u trÃºc trong tÆ°Æ¡ng lai.
st.markdown("""
<style>
/* Target the container holding the text input */
/* Adjust selector if needed based on Streamlit version changes */
div.stTextInput {
    position: fixed; /* Cá»‘ Ä‘á»‹nh vá»‹ trÃ­ */
    bottom: 1rem; /* Khoáº£ng cÃ¡ch tá»« Ä‘Ã¡y mÃ n hÃ¬nh */
    left: 50%; /* CÄƒn giá»¯a theo chiá»u ngang */
    transform: translateX(-50%); /* Äiá»u chá»‰nh cÄƒn giá»¯a chÃ­nh xÃ¡c */
    width: calc(100% - 4rem); /* Chiá»u rá»™ng, trá»« Ä‘i padding hai bÃªn */
    max-width: 736px; /* Giá»›i háº¡n chiá»u rá»™ng tá»‘i Ä‘a giá»‘ng ná»™i dung chÃ­nh */
    padding: 0.75rem 1rem;
    background-color: #ffffff; /* MÃ u ná»n tráº¯ng (hoáº·c mÃ u ná»n theme) */
    border-top: 1px solid #e6e6e6; /* ÄÆ°á»ng viá»n má» phÃ­a trÃªn */
    border-radius: 0.5rem; /* Bo gÃ³c nháº¹ */
    box-shadow: 0 -2px 5px rgba(0,0,0,0.05); /* Äá»• bÃ³ng nháº¹ */
    z-index: 99; /* Äáº£m báº£o náº±m trÃªn cÃ¡c thÃ nh pháº§n khÃ¡c */
}

/* ThÃªm padding vÃ o cuá»‘i ná»™i dung chÃ­nh Ä‘á»ƒ khÃ´ng bá»‹ Ã´ input che máº¥t */
.main .block-container {
    padding-bottom: 6rem; /* TÄƒng padding Ä‘á»ƒ Ä‘á»§ chá»— cho Ã´ input cá»‘ Ä‘á»‹nh */
}
</style>
""", unsafe_allow_html=True)


# --- Táº£i dá»¯ liá»‡u nhÃ¢n sá»± tá»« file JSON ---
JSON_FILE_PATH = "data.json"

@st.cache_data
def load_knowledge(file_path):
    """Táº£i dá»¯ liá»‡u JSON"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        st.sidebar.error(f"âŒ KhÃ´ng tÃ¬m tháº¥y file '{file_path}'!")
        return None
    except json.JSONDecodeError:
        st.sidebar.error(f"âŒ File '{file_path}' khÃ´ng Ä‘Ãºng Ä‘á»‹nh dáº¡ng JSON!")
        return None
    except Exception as e:
        st.sidebar.error(f"âŒ Lá»—i khi táº£i dá»¯ liá»‡u: {e}")
        return None

knowledge_data = load_knowledge(JSON_FILE_PATH)

if knowledge_data is None:
    st.error("ğŸš¨ KhÃ´ng thá»ƒ khá»Ÿi Ä‘á»™ng á»©ng dá»¥ng do lá»—i khi táº£i dá»¯ liá»‡u. Vui lÃ²ng kiá»ƒm tra file data.json!")
    st.stop()
else:
    st.sidebar.success(f"âœ… ÄÃ£ táº£i dá»¯ liá»‡u tá»« {JSON_FILE_PATH}")

# --- Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u thÃ nh dáº¡ng chuá»—i cho Prompt ---
# Chá»‰ dÃ¹ng JSON trong phiÃªn báº£n nÃ y
knowledge_base_string = ""
if knowledge_data:
    json_string_compact = json.dumps(knowledge_data, ensure_ascii=False, indent=None)
    knowledge_base_string += f"Dá»¯ liá»‡u tá»« JSON:\n```json\n{json_string_compact}\n```\n\n"
else:
     st.error("ğŸš¨ KhÃ´ng cÃ³ dá»¯ liá»‡u kiáº¿n thá»©c Ä‘á»ƒ táº¡o prompt.")
     st.stop()


# --- Cáº¥u hÃ¬nh Google Generative AI ---
api_key = os.environ.get("GEMINI_API_KEY") or st.secrets.get("GOOGLE_API_KEY")

if not api_key:
    st.error("ğŸš¨ API Key chÆ°a Ä‘Æ°á»£c thiáº¿t láº­p!")
    st.stop()

try:
    genai.configure(api_key=api_key)
except Exception as e:
    st.error(f"ğŸš¨ Lá»—i khi cáº¥u hÃ¬nh Google AI vá»›i API Key: {e}")
    st.stop()

# --- Khá»Ÿi táº¡o Model AI ---
MODEL_NAME = "gemini-2.5-pro-exp-03-25" # Äá»•i láº¡i model cÃ´ng khai, á»•n Ä‘á»‹nh
try:
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    ]
    model = genai.GenerativeModel(MODEL_NAME, safety_settings=safety_settings)
    st.sidebar.info(f"âœ… Äang sá»­ dá»¥ng model: {MODEL_NAME}")
except Exception as e:
    st.error(f"ğŸš¨ Lá»—i khi khá»Ÿi táº¡o model '{MODEL_NAME}': {e}")
    st.stop()

# --- System Prompt ---
system_instruction_text = """
Báº¡n lÃ  trá»£ lÃ½ nhÃ¢n sá»± cá»§a cÃ´ng ty INNO. Báº¡n chá»‰ Ä‘Æ°á»£c sá»­ dá»¥ng dá»¯ liá»‡u Ä‘Æ°á»£c cung cáº¥p dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.
Náº¿u thÃ´ng tin khÃ´ng cÃ³ trong dá»¯ liá»‡u, hÃ£y tráº£ lá»i: "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong dá»¯ liá»‡u cÃ³ sáºµn."
Äá»«ng Ä‘Æ°a ra suy Ä‘oÃ¡n hoáº·c thÃ´ng tin khÃ´ng cháº¯c cháº¯n. Tráº£ lá»i ngáº¯n gá»n, Ä‘Ãºng trá»ng tÃ¢m.
"""

generation_config = genai.types.GenerationConfig(
    response_mime_type="text/plain",
    temperature=0.7
)

# --- Xá»­ lÃ½ pháº£n há»“i dáº¡ng Stream ---
def stream_text_generator(stream):
    """Nháº­n stream tá»« API vÃ  chá»‰ xuáº¥t pháº§n text"""
    for chunk in stream:
        if hasattr(chunk, 'text') and chunk.text:
            yield chunk.text

# --- Quáº£n lÃ½ Lá»‹ch sá»­ Chat (Session State) ---
# Khá»Ÿi táº¡o danh sÃ¡ch tin nháº¯n náº¿u chÆ°a cÃ³
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- Hiá»ƒn thá»‹ Lá»‹ch sá»­ Chat ---
st.markdown("### Lá»‹ch sá»­ trÃ² chuyá»‡n")
# Táº¡o container Ä‘á»ƒ chá»©a lá»‹ch sá»­, cho phÃ©p cuá»™n náº¿u cáº§n
# KhÃ´ng cáº§n giá»›i háº¡n chiá»u cao cá»‘ Ä‘á»‹nh náº¿u Ã´ input Ä‘Ã£ cá»‘ Ä‘á»‹nh á»Ÿ dÆ°á»›i
chat_container = st.container()

with chat_container:
    # Chá»‰ hiá»ƒn thá»‹ tá»‘i Ä‘a 3 cáº·p há»i Ä‘Ã¡p gáº§n nháº¥t (6 tin nháº¯n)
    history_limit = 6
    start_index = max(0, len(st.session_state.messages) - history_limit)
    for i in range(start_index, len(st.session_state.messages)):
        message = st.session_state.messages[i]
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# --- Giao diá»‡n nháº­p cÃ¢u há»i (Sáº½ Ä‘Æ°á»£c CSS di chuyá»ƒn xuá»‘ng dÆ°á»›i) ---
user_question = st.text_input(
    "Nháº­p cÃ¢u há»i cá»§a báº¡n:",
    key="user_question_input", # Key Ä‘á»ƒ quáº£n lÃ½ input
    placeholder="VÃ­ dá»¥: Cho tÃ´i biáº¿t email cá»§a Nguyá»…n VÄƒn A?",
    label_visibility="collapsed" # áº¨n label máº·c Ä‘á»‹nh Ä‘á»ƒ tiáº¿t kiá»‡m khÃ´ng gian
)

# --- Xá»­ lÃ½ logic chÃ­nh khi cÃ³ cÃ¢u há»i má»›i ---
if user_question:
    # 1. ThÃªm cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng vÃ o lá»‹ch sá»­ vÃ  hiá»ƒn thá»‹ ngay láº­p tá»©c
    # Kiá»ƒm tra xem tin nháº¯n cuá»‘i cÃ¹ng cÃ³ pháº£i lÃ  cÃ¢u há»i nÃ y khÃ´ng Ä‘á»ƒ trÃ¡nh láº·p láº¡i khi rerun
    if not st.session_state.messages or st.session_state.messages[-1].get("content") != user_question:
        st.session_state.messages.append({"role": "user", "content": user_question})

        # Hiá»ƒn thá»‹ cÃ¢u há»i má»›i nháº¥t trong container lá»‹ch sá»­
        with chat_container:
             with st.chat_message("user"):
                 st.markdown(user_question)

        # 2. XÃ¢y dá»±ng prompt vÃ  gá»i API
        full_prompt = f"{system_instruction_text}\n\nDÆ°á»›i Ä‘Ã¢y lÃ  dá»¯ liá»‡u nhÃ¢n sá»±:\n\n{knowledge_base_string}\n\nHÃ£y tráº£ lá»i cÃ¢u há»i sau:\n\"{user_question}\""
        contents = [full_prompt]

        # 3. Gá»i API vÃ  xá»­ lÃ½ pháº£n há»“i
        try:
            # Hiá»ƒn thá»‹ spinner ngay dÆ°á»›i cÃ¢u há»i má»›i
            with chat_container:
                with st.spinner(f"ğŸ” Äang tÃ¬m kiáº¿m cÃ¢u tráº£ lá»i vá»›i {MODEL_NAME}..."):
                    response_stream = model.generate_content(
                        contents,
                        stream=True,
                        generation_config=generation_config
                    )

                    # 4. Hiá»ƒn thá»‹ cÃ¢u tráº£ lá»i dáº¡ng stream vÃ  lÆ°u trá»¯
                    with st.chat_message("assistant"):
                        full_response = ""
                        text_stream_placeholder = st.empty()
                        text_generator = stream_text_generator(response_stream)
                        for chunk in text_generator:
                            full_response += chunk
                            text_stream_placeholder.markdown(full_response + "â–Œ") # Con trá» nháº¥p nhÃ¡y
                        text_stream_placeholder.markdown(full_response) # Hiá»ƒn thá»‹ Ä‘áº§y Ä‘á»§

                # 5. ThÃªm cÃ¢u tráº£ lá»i Ä‘áº§y Ä‘á»§ vÃ o lá»‹ch sá»­ (sau khi stream xong)
                st.session_state.messages.append({"role": "assistant", "content": full_response})

                # XÃ³a ná»™i dung Ã´ input báº±ng cÃ¡ch set láº¡i key cá»§a nÃ³ trong session_state
                # Cáº§n thá»±c hiá»‡n á»Ÿ cuá»‘i Ä‘á»ƒ khÃ´ng gÃ¢y rerun khÃ´ng mong muá»‘n trÆ°á»›c khi lÆ°u state
                # st.session_state.user_question_input = "" # Bá» comment dÃ²ng nÃ y náº¿u muá»‘n tá»± xÃ³a input

                # Cháº¡y láº¡i Ä‘á»ƒ cáº­p nháº­t hiá»ƒn thá»‹ lá»‹ch sá»­ Ä‘áº§y Ä‘á»§ (tÃ¹y chá»n, cÃ³ thá»ƒ hÆ¡i giáº­t)
                # st.rerun()

        except Exception as e:
            st.error(f"ğŸš¨ ÄÃ£ xáº£y ra lá»—i khi gá»i Gemini API: {e}")
            print(f"Error calling Gemini API: {e}")

# --- ThÃ´ng tin thÃªm ---
# CÃ³ thá»ƒ khÃ´ng cáº§n thiáº¿t ná»¯a náº¿u giao diá»‡n táº­p trung vÃ o chat
# st.sidebar.markdown("---")
# st.sidebar.caption("Â© 2025 - Beta App v0.6")
